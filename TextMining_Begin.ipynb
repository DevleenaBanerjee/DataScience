{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "words=nltk.word_tokenize(paragraph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(k) for k in words]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you all so veri much .',\n",
       " 'thank you to the academi .',\n",
       " 'thank you to all of you in thi room .',\n",
       " 'I have to congratul the other incred nomine thi year .',\n",
       " 'the reven wa the product of the tireless effort of an unbeliev cast and crew .',\n",
       " 'first off , to my brother in thi endeavor , mr. tom hardi .',\n",
       " 'tom , your talent on screen can onli be surpass by your friendship off screen … thank you for creat a t ranscend cinemat experi .',\n",
       " 'thank you to everybodi at fox and new regenc … my entir team .',\n",
       " 'I have to thank everyon from the veri onset of my career … To my parent ; none of thi would be possibl without you .',\n",
       " 'and to my friend , I love you dearli ; you know who you are .',\n",
       " \"and lastli , I just want to say thi : make the reven wa about man 's relationship to the natur world .\",\n",
       " 'A world that we collect felt in 2015 as the hottest year in record histori .',\n",
       " 'our product need to move to the southern tip of thi planet just to be abl to find snow .',\n",
       " 'climat chang is real , it is happen right now .',\n",
       " 'It is the most urgent threat face our entir speci , and we need to work collect togeth and stop procrastin .',\n",
       " 'We need to support leader around the world who do not speak for the big pollut , but who speak for all of human , for the indigen peopl of the world , for the billion and billion of underprivileg peopl out there who would be most affect by thi .',\n",
       " 'for our children ’ s children , and for those peopl out there whose voic have been drown out by the polit of greed .',\n",
       " 'I thank you all for thi amaz award tonight .',\n",
       " 'let us not take thi planet for grant .',\n",
       " 'I do not take tonight for grant .',\n",
       " 'thank you so veri much .']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\amhotta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    #newwords=[lemmatizer.lemmatize(k) for k in words]\n",
    "    newwords=[]\n",
    "    for k in words:\n",
    "        newwords.append(lemmatizer.lemmatize(k))\n",
    "    sentences[i]=' '.join(newwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you all so veri much .',\n",
       " 'thank you to the academi .',\n",
       " 'thank you to all of you in thi room .',\n",
       " 'I have to congratul the other incred nomine thi year .',\n",
       " 'the reven wa the product of the tireless effort of an unbeliev cast and crew .',\n",
       " 'first off , to my brother in thi endeavor , mr. tom hardi .',\n",
       " 'tom , your talent on screen can onli be surpass by your friendship off screen … thank you for creat a t ranscend cinemat experi .',\n",
       " 'thank you to everybodi at fox and new regenc … my entir team .',\n",
       " 'I have to thank everyon from the veri onset of my career … To my parent ; none of thi would be possibl without you .',\n",
       " 'and to my friend , I love you dearli ; you know who you are .',\n",
       " \"and lastli , I just want to say thi : make the reven wa about man 's relationship to the natur world .\",\n",
       " 'A world that we collect felt in 2015 a the hottest year in record histori .',\n",
       " 'our product need to move to the southern tip of thi planet just to be abl to find snow .',\n",
       " 'climat chang is real , it is happen right now .',\n",
       " 'It is the most urgent threat face our entir speci , and we need to work collect togeth and stop procrastin .',\n",
       " 'We need to support leader around the world who do not speak for the big pollut , but who speak for all of human , for the indigen peopl of the world , for the billion and billion of underprivileg peopl out there who would be most affect by thi .',\n",
       " 'for our child ’ s child , and for those peopl out there whose voic have been drown out by the polit of greed .',\n",
       " 'I thank you all for thi amaz award tonight .',\n",
       " 'let u not take thi planet for grant .',\n",
       " 'I do not take tonight for grant .',\n",
       " 'thank you so veri much .']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\amhotta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    newwords=[word for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i]=' '.join(newwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank veri much .',\n",
       " 'thank academi .',\n",
       " 'thank thi room .',\n",
       " 'I congratul incred nomine thi year .',\n",
       " 'reven wa product tireless effort unbeliev cast crew .',\n",
       " 'first , brother thi endeavor , mr. tom hardi .',\n",
       " 'tom , talent screen onli surpass friendship screen … thank creat ranscend cinemat experi .',\n",
       " 'thank everybodi fox new regenc … entir team .',\n",
       " 'I thank everyon veri onset career … To parent ; none thi would possibl without .',\n",
       " 'friend , I love dearli ; know .',\n",
       " \"lastli , I want say thi : make reven wa man 's relationship natur world .\",\n",
       " 'A world collect felt 2015 hottest year record histori .',\n",
       " 'product need move southern tip thi planet abl find snow .',\n",
       " 'climat chang real , happen right .',\n",
       " 'It urgent threat face entir speci , need work collect togeth stop procrastin .',\n",
       " 'We need support leader around world speak big pollut , speak human , indigen peopl world , billion billion underprivileg peopl would affect thi .',\n",
       " 'child ’ child , peopl whose voic drown polit greed .',\n",
       " 'I thank thi amaz award tonight .',\n",
       " 'let u take thi planet grant .',\n",
       " 'I take tonight grant .',\n",
       " 'thank veri much .']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\amhotta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('so', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('Academy', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('room', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('congratulate', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('incredible', 'JJ'),\n",
       " ('nominees', 'NNS'),\n",
       " ('this', 'DT'),\n",
       " ('year', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('Revenant', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('product', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('tireless', 'NN'),\n",
       " ('efforts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('unbelievable', 'JJ'),\n",
       " ('cast', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('crew', 'NN'),\n",
       " ('.', '.'),\n",
       " ('First', 'NNP'),\n",
       " ('off', 'RB'),\n",
       " (',', ','),\n",
       " ('to', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('brother', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('endeavor', 'NN'),\n",
       " (',', ','),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Tom', 'NNP'),\n",
       " ('Hardy', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Tom', 'NNP'),\n",
       " (',', ','),\n",
       " ('your', 'PRP$'),\n",
       " ('talent', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('screen', 'NN'),\n",
       " ('can', 'MD'),\n",
       " ('only', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('surpassed', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('your', 'PRP$'),\n",
       " ('friendship', 'NN'),\n",
       " ('off', 'IN'),\n",
       " ('screen', 'JJ'),\n",
       " ('…', 'NNP'),\n",
       " ('thank', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('for', 'IN'),\n",
       " ('creating', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('t', 'JJ'),\n",
       " ('ranscendent', 'NN'),\n",
       " ('cinematic', 'JJ'),\n",
       " ('experience', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('everybody', 'VB'),\n",
       " ('at', 'IN'),\n",
       " ('Fox', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('New', 'NNP'),\n",
       " ('Regency', 'NNP'),\n",
       " ('…', 'NNP'),\n",
       " ('my', 'PRP$'),\n",
       " ('entire', 'JJ'),\n",
       " ('team', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('thank', 'VB'),\n",
       " ('everyone', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('onset', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('career', 'NN'),\n",
       " ('…', 'NN'),\n",
       " ('To', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('parents', 'NNS'),\n",
       " (';', ':'),\n",
       " ('none', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('possible', 'JJ'),\n",
       " ('without', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('to', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('friends', 'NNS'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('dearly', 'RB'),\n",
       " (';', ':'),\n",
       " ('you', 'PRP'),\n",
       " ('know', 'VBP'),\n",
       " ('who', 'WP'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('lastly', 'RB'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('say', 'VB'),\n",
       " ('this', 'DT'),\n",
       " (':', ':'),\n",
       " ('Making', 'VBG'),\n",
       " ('The', 'DT'),\n",
       " ('Revenant', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('about', 'IN'),\n",
       " ('man', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('relationship', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('natural', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('.', '.'),\n",
       " ('A', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('collectively', 'RB'),\n",
       " ('felt', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('2015', 'CD'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('hottest', 'JJS'),\n",
       " ('year', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('recorded', 'JJ'),\n",
       " ('history', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRP$'),\n",
       " ('production', 'NN'),\n",
       " ('needed', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('move', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('southern', 'JJ'),\n",
       " ('tip', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('planet', 'NN'),\n",
       " ('just', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('find', 'VB'),\n",
       " ('snow', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Climate', 'NNP'),\n",
       " ('change', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('real', 'JJ'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('happening', 'VBG'),\n",
       " ('right', 'RB'),\n",
       " ('now', 'RB'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('urgent', 'JJ'),\n",
       " ('threat', 'NN'),\n",
       " ('facing', 'VBG'),\n",
       " ('our', 'PRP$'),\n",
       " ('entire', 'JJ'),\n",
       " ('species', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('we', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('work', 'VB'),\n",
       " ('collectively', 'RB'),\n",
       " ('together', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('stop', 'VB'),\n",
       " ('procrastinating', 'NN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('support', 'VB'),\n",
       " ('leaders', 'NNS'),\n",
       " ('around', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('speak', 'VB'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('polluters', 'NNS'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('who', 'WP'),\n",
       " ('speak', 'VBP'),\n",
       " ('for', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('humanity', 'NN'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('indigenous', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('billions', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('billions', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('underprivileged', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('out', 'IN'),\n",
       " ('there', 'EX'),\n",
       " ('who', 'WP'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('most', 'RBS'),\n",
       " ('affected', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('children', 'NNS'),\n",
       " ('’', 'VBP'),\n",
       " ('s', 'JJ'),\n",
       " ('children', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('for', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('people', 'NNS'),\n",
       " ('out', 'RP'),\n",
       " ('there', 'RB'),\n",
       " ('whose', 'WP$'),\n",
       " ('voices', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('drowned', 'VBN'),\n",
       " ('out', 'RP'),\n",
       " ('by', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('politics', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('greed', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('thank', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('for', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('amazing', 'JJ'),\n",
       " ('award', 'NN'),\n",
       " ('tonight', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Let', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('not', 'RB'),\n",
       " ('take', 'VB'),\n",
       " ('this', 'DT'),\n",
       " ('planet', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('granted', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('take', 'VB'),\n",
       " ('tonight', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('granted', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('so', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words=nltk.pos_tag(words)\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tags=[]\n",
    "for tw in tagged_words:\n",
    "    word_tags.append(tw[0]+\"_\"+tw[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank_NNP',\n",
       " 'you_PRP',\n",
       " 'all_DT',\n",
       " 'so_RB',\n",
       " 'very_RB',\n",
       " 'much_JJ',\n",
       " '._.',\n",
       " 'Thank_VB',\n",
       " 'you_PRP',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'Academy_NNP',\n",
       " '._.',\n",
       " 'Thank_NNP',\n",
       " 'you_PRP',\n",
       " 'to_TO',\n",
       " 'all_DT',\n",
       " 'of_IN',\n",
       " 'you_PRP',\n",
       " 'in_IN',\n",
       " 'this_DT',\n",
       " 'room_NN',\n",
       " '._.',\n",
       " 'I_PRP',\n",
       " 'have_VBP',\n",
       " 'to_TO',\n",
       " 'congratulate_VB',\n",
       " 'the_DT',\n",
       " 'other_JJ',\n",
       " 'incredible_JJ',\n",
       " 'nominees_NNS',\n",
       " 'this_DT',\n",
       " 'year_NN',\n",
       " '._.',\n",
       " 'The_DT',\n",
       " 'Revenant_NNP',\n",
       " 'was_VBD',\n",
       " 'the_DT',\n",
       " 'product_NN',\n",
       " 'of_IN',\n",
       " 'the_DT',\n",
       " 'tireless_NN',\n",
       " 'efforts_NNS',\n",
       " 'of_IN',\n",
       " 'an_DT',\n",
       " 'unbelievable_JJ',\n",
       " 'cast_NN',\n",
       " 'and_CC',\n",
       " 'crew_NN',\n",
       " '._.',\n",
       " 'First_NNP',\n",
       " 'off_RB',\n",
       " ',_,',\n",
       " 'to_TO',\n",
       " 'my_PRP$',\n",
       " 'brother_NN',\n",
       " 'in_IN',\n",
       " 'this_DT',\n",
       " 'endeavor_NN',\n",
       " ',_,',\n",
       " 'Mr._NNP',\n",
       " 'Tom_NNP',\n",
       " 'Hardy_NNP',\n",
       " '._.',\n",
       " 'Tom_NNP',\n",
       " ',_,',\n",
       " 'your_PRP$',\n",
       " 'talent_NN',\n",
       " 'on_IN',\n",
       " 'screen_NN',\n",
       " 'can_MD',\n",
       " 'only_RB',\n",
       " 'be_VB',\n",
       " 'surpassed_VBN',\n",
       " 'by_IN',\n",
       " 'your_PRP$',\n",
       " 'friendship_NN',\n",
       " 'off_IN',\n",
       " 'screen_JJ',\n",
       " '…_NNP',\n",
       " 'thank_NN',\n",
       " 'you_PRP',\n",
       " 'for_IN',\n",
       " 'creating_VBG',\n",
       " 'a_DT',\n",
       " 't_JJ',\n",
       " 'ranscendent_NN',\n",
       " 'cinematic_JJ',\n",
       " 'experience_NN',\n",
       " '._.',\n",
       " 'Thank_NNP',\n",
       " 'you_PRP',\n",
       " 'to_TO',\n",
       " 'everybody_VB',\n",
       " 'at_IN',\n",
       " 'Fox_NNP',\n",
       " 'and_CC',\n",
       " 'New_NNP',\n",
       " 'Regency_NNP',\n",
       " '…_NNP',\n",
       " 'my_PRP$',\n",
       " 'entire_JJ',\n",
       " 'team_NN',\n",
       " '._.',\n",
       " 'I_PRP',\n",
       " 'have_VBP',\n",
       " 'to_TO',\n",
       " 'thank_VB',\n",
       " 'everyone_NN',\n",
       " 'from_IN',\n",
       " 'the_DT',\n",
       " 'very_RB',\n",
       " 'onset_NN',\n",
       " 'of_IN',\n",
       " 'my_PRP$',\n",
       " 'career_NN',\n",
       " '…_NN',\n",
       " 'To_TO',\n",
       " 'my_PRP$',\n",
       " 'parents_NNS',\n",
       " ';_:',\n",
       " 'none_NN',\n",
       " 'of_IN',\n",
       " 'this_DT',\n",
       " 'would_MD',\n",
       " 'be_VB',\n",
       " 'possible_JJ',\n",
       " 'without_IN',\n",
       " 'you_PRP',\n",
       " '._.',\n",
       " 'And_CC',\n",
       " 'to_TO',\n",
       " 'my_PRP$',\n",
       " 'friends_NNS',\n",
       " ',_,',\n",
       " 'I_PRP',\n",
       " 'love_VBP',\n",
       " 'you_PRP',\n",
       " 'dearly_RB',\n",
       " ';_:',\n",
       " 'you_PRP',\n",
       " 'know_VBP',\n",
       " 'who_WP',\n",
       " 'you_PRP',\n",
       " 'are_VBP',\n",
       " '._.',\n",
       " 'And_CC',\n",
       " 'lastly_RB',\n",
       " ',_,',\n",
       " 'I_PRP',\n",
       " 'just_RB',\n",
       " 'want_VBP',\n",
       " 'to_TO',\n",
       " 'say_VB',\n",
       " 'this_DT',\n",
       " ':_:',\n",
       " 'Making_VBG',\n",
       " 'The_DT',\n",
       " 'Revenant_NNP',\n",
       " 'was_VBD',\n",
       " 'about_IN',\n",
       " 'man_NN',\n",
       " \"'s_POS\",\n",
       " 'relationship_NN',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'natural_JJ',\n",
       " 'world_NN',\n",
       " '._.',\n",
       " 'A_DT',\n",
       " 'world_NN',\n",
       " 'that_IN',\n",
       " 'we_PRP',\n",
       " 'collectively_RB',\n",
       " 'felt_VBD',\n",
       " 'in_IN',\n",
       " '2015_CD',\n",
       " 'as_IN',\n",
       " 'the_DT',\n",
       " 'hottest_JJS',\n",
       " 'year_NN',\n",
       " 'in_IN',\n",
       " 'recorded_JJ',\n",
       " 'history_NN',\n",
       " '._.',\n",
       " 'Our_PRP$',\n",
       " 'production_NN',\n",
       " 'needed_VBN',\n",
       " 'to_TO',\n",
       " 'move_VB',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'southern_JJ',\n",
       " 'tip_NN',\n",
       " 'of_IN',\n",
       " 'this_DT',\n",
       " 'planet_NN',\n",
       " 'just_RB',\n",
       " 'to_TO',\n",
       " 'be_VB',\n",
       " 'able_JJ',\n",
       " 'to_TO',\n",
       " 'find_VB',\n",
       " 'snow_JJ',\n",
       " '._.',\n",
       " 'Climate_NNP',\n",
       " 'change_NN',\n",
       " 'is_VBZ',\n",
       " 'real_JJ',\n",
       " ',_,',\n",
       " 'it_PRP',\n",
       " 'is_VBZ',\n",
       " 'happening_VBG',\n",
       " 'right_RB',\n",
       " 'now_RB',\n",
       " '._.',\n",
       " 'It_PRP',\n",
       " 'is_VBZ',\n",
       " 'the_DT',\n",
       " 'most_RBS',\n",
       " 'urgent_JJ',\n",
       " 'threat_NN',\n",
       " 'facing_VBG',\n",
       " 'our_PRP$',\n",
       " 'entire_JJ',\n",
       " 'species_NNS',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'we_PRP',\n",
       " 'need_VBP',\n",
       " 'to_TO',\n",
       " 'work_VB',\n",
       " 'collectively_RB',\n",
       " 'together_RB',\n",
       " 'and_CC',\n",
       " 'stop_VB',\n",
       " 'procrastinating_NN',\n",
       " '._.',\n",
       " 'We_PRP',\n",
       " 'need_VBP',\n",
       " 'to_TO',\n",
       " 'support_VB',\n",
       " 'leaders_NNS',\n",
       " 'around_IN',\n",
       " 'the_DT',\n",
       " 'world_NN',\n",
       " 'who_WP',\n",
       " 'do_VBP',\n",
       " 'not_RB',\n",
       " 'speak_VB',\n",
       " 'for_IN',\n",
       " 'the_DT',\n",
       " 'big_JJ',\n",
       " 'polluters_NNS',\n",
       " ',_,',\n",
       " 'but_CC',\n",
       " 'who_WP',\n",
       " 'speak_VBP',\n",
       " 'for_IN',\n",
       " 'all_DT',\n",
       " 'of_IN',\n",
       " 'humanity_NN',\n",
       " ',_,',\n",
       " 'for_IN',\n",
       " 'the_DT',\n",
       " 'indigenous_JJ',\n",
       " 'people_NNS',\n",
       " 'of_IN',\n",
       " 'the_DT',\n",
       " 'world_NN',\n",
       " ',_,',\n",
       " 'for_IN',\n",
       " 'the_DT',\n",
       " 'billions_NNS',\n",
       " 'and_CC',\n",
       " 'billions_NNS',\n",
       " 'of_IN',\n",
       " 'underprivileged_JJ',\n",
       " 'people_NNS',\n",
       " 'out_IN',\n",
       " 'there_EX',\n",
       " 'who_WP',\n",
       " 'would_MD',\n",
       " 'be_VB',\n",
       " 'most_RBS',\n",
       " 'affected_VBN',\n",
       " 'by_IN',\n",
       " 'this_DT',\n",
       " '._.',\n",
       " 'For_IN',\n",
       " 'our_PRP$',\n",
       " 'children_NNS',\n",
       " '’_VBP',\n",
       " 's_JJ',\n",
       " 'children_NNS',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'for_IN',\n",
       " 'those_DT',\n",
       " 'people_NNS',\n",
       " 'out_RP',\n",
       " 'there_RB',\n",
       " 'whose_WP$',\n",
       " 'voices_NNS',\n",
       " 'have_VBP',\n",
       " 'been_VBN',\n",
       " 'drowned_VBN',\n",
       " 'out_RP',\n",
       " 'by_IN',\n",
       " 'the_DT',\n",
       " 'politics_NNS',\n",
       " 'of_IN',\n",
       " 'greed_NN',\n",
       " '._.',\n",
       " 'I_PRP',\n",
       " 'thank_VBP',\n",
       " 'you_PRP',\n",
       " 'all_DT',\n",
       " 'for_IN',\n",
       " 'this_DT',\n",
       " 'amazing_JJ',\n",
       " 'award_NN',\n",
       " 'tonight_NN',\n",
       " '._.',\n",
       " 'Let_VB',\n",
       " 'us_PRP',\n",
       " 'not_RB',\n",
       " 'take_VB',\n",
       " 'this_DT',\n",
       " 'planet_NN',\n",
       " 'for_IN',\n",
       " 'granted_VBN',\n",
       " '._.',\n",
       " 'I_PRP',\n",
       " 'do_VBP',\n",
       " 'not_RB',\n",
       " 'take_VB',\n",
       " 'tonight_NN',\n",
       " 'for_IN',\n",
       " 'granted_VBN',\n",
       " '._.',\n",
       " 'Thank_NNP',\n",
       " 'you_PRP',\n",
       " 'so_RB',\n",
       " 'very_RB',\n",
       " 'much_JJ',\n",
       " '._.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_paragrapgh=\" \".join(word_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"The Taj Mahal was built by Emperor Shah Jahan\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Taj', 'NNP'),\n",
       " ('Mahal', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('built', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Emperor', 'NNP'),\n",
       " ('Shah', 'NNP'),\n",
       " ('Jahan', 'NNP')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\amhotta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\amhotta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEnt= nltk.ne_chunk(tagged_words)\n",
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i]=dataset[i].lower()\n",
    "    dataset[i]=re.sub(r'\\W',' ',dataset[i])\n",
    "    dataset[i]=re.sub(r'\\s+',' ',dataset[i])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating histogram\n",
    "word2Count = {}\n",
    "for data in dataset:\n",
    "    words=nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2Count.keys():\n",
    "            word2Count[word]=1\n",
    "        else:\n",
    "            word2Count[word]+=1\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 8,\n",
       " 'you': 12,\n",
       " 'all': 4,\n",
       " 'so': 2,\n",
       " 'very': 3,\n",
       " 'much': 2,\n",
       " 'to': 16,\n",
       " 'the': 17,\n",
       " 'academy': 1,\n",
       " 'of': 10,\n",
       " 'in': 4,\n",
       " 'this': 9,\n",
       " 'room': 1,\n",
       " 'i': 6,\n",
       " 'have': 3,\n",
       " 'congratulate': 1,\n",
       " 'other': 1,\n",
       " 'incredible': 1,\n",
       " 'nominees': 1,\n",
       " 'year': 2,\n",
       " 'revenant': 2,\n",
       " 'was': 2,\n",
       " 'product': 1,\n",
       " 'tireless': 1,\n",
       " 'efforts': 1,\n",
       " 'an': 1,\n",
       " 'unbelievable': 1,\n",
       " 'cast': 1,\n",
       " 'and': 8,\n",
       " 'crew': 1,\n",
       " 'first': 1,\n",
       " 'off': 2,\n",
       " 'my': 5,\n",
       " 'brother': 1,\n",
       " 'endeavor': 1,\n",
       " 'mr': 1,\n",
       " 'tom': 2,\n",
       " 'hardy': 1,\n",
       " 'your': 2,\n",
       " 'talent': 1,\n",
       " 'on': 1,\n",
       " 'screen': 2,\n",
       " 'can': 1,\n",
       " 'only': 1,\n",
       " 'be': 4,\n",
       " 'surpassed': 1,\n",
       " 'by': 3,\n",
       " 'friendship': 1,\n",
       " 'for': 10,\n",
       " 'creating': 1,\n",
       " 'a': 2,\n",
       " 't': 1,\n",
       " 'ranscendent': 1,\n",
       " 'cinematic': 1,\n",
       " 'experience': 1,\n",
       " 'everybody': 1,\n",
       " 'at': 1,\n",
       " 'fox': 1,\n",
       " 'new': 1,\n",
       " 'regency': 1,\n",
       " 'entire': 2,\n",
       " 'team': 1,\n",
       " 'everyone': 1,\n",
       " 'from': 1,\n",
       " 'onset': 1,\n",
       " 'career': 1,\n",
       " 'parents': 1,\n",
       " 'none': 1,\n",
       " 'would': 2,\n",
       " 'possible': 1,\n",
       " 'without': 1,\n",
       " 'friends': 1,\n",
       " 'love': 1,\n",
       " 'dearly': 1,\n",
       " 'know': 1,\n",
       " 'who': 4,\n",
       " 'are': 1,\n",
       " 'lastly': 1,\n",
       " 'just': 2,\n",
       " 'want': 1,\n",
       " 'say': 1,\n",
       " 'making': 1,\n",
       " 'about': 1,\n",
       " 'man': 1,\n",
       " 's': 2,\n",
       " 'relationship': 1,\n",
       " 'natural': 1,\n",
       " 'world': 4,\n",
       " 'that': 1,\n",
       " 'we': 3,\n",
       " 'collectively': 2,\n",
       " 'felt': 1,\n",
       " '2015': 1,\n",
       " 'as': 1,\n",
       " 'hottest': 1,\n",
       " 'recorded': 1,\n",
       " 'history': 1,\n",
       " 'our': 3,\n",
       " 'production': 1,\n",
       " 'needed': 1,\n",
       " 'move': 1,\n",
       " 'southern': 1,\n",
       " 'tip': 1,\n",
       " 'planet': 2,\n",
       " 'able': 1,\n",
       " 'find': 1,\n",
       " 'snow': 1,\n",
       " 'climate': 1,\n",
       " 'change': 1,\n",
       " 'is': 3,\n",
       " 'real': 1,\n",
       " 'it': 2,\n",
       " 'happening': 1,\n",
       " 'right': 1,\n",
       " 'now': 1,\n",
       " 'most': 2,\n",
       " 'urgent': 1,\n",
       " 'threat': 1,\n",
       " 'facing': 1,\n",
       " 'species': 1,\n",
       " 'need': 2,\n",
       " 'work': 1,\n",
       " 'together': 1,\n",
       " 'stop': 1,\n",
       " 'procrastinating': 1,\n",
       " 'support': 1,\n",
       " 'leaders': 1,\n",
       " 'around': 1,\n",
       " 'do': 2,\n",
       " 'not': 3,\n",
       " 'speak': 2,\n",
       " 'big': 1,\n",
       " 'polluters': 1,\n",
       " 'but': 1,\n",
       " 'humanity': 1,\n",
       " 'indigenous': 1,\n",
       " 'people': 3,\n",
       " 'billions': 2,\n",
       " 'underprivileged': 1,\n",
       " 'out': 3,\n",
       " 'there': 2,\n",
       " 'affected': 1,\n",
       " 'children': 2,\n",
       " 'those': 1,\n",
       " 'whose': 1,\n",
       " 'voices': 1,\n",
       " 'been': 1,\n",
       " 'drowned': 1,\n",
       " 'politics': 1,\n",
       " 'greed': 1,\n",
       " 'amazing': 1,\n",
       " 'award': 1,\n",
       " 'tonight': 2,\n",
       " 'let': 1,\n",
       " 'us': 1,\n",
       " 'take': 2,\n",
       " 'granted': 2}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = heapq.nlargest(100,word2Count,key=word2Count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'you', 'of', 'for', 'this', 'thank', 'and', 'i', 'my']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TF IDF Model\n",
    "import numpy as np\n",
    "word_idfs={}\n",
    "for word in freq_words:\n",
    "    doc_count=0\n",
    "    for data in dataset:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            doc_count+=1\n",
    "    word_idfs[word]=np.log((len(dataset)/doc_count)+1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1.1314021114911006,\n",
       " 'to': 1.067840630001356,\n",
       " 'you': 1.2039728043259361,\n",
       " 'of': 1.5040773967762742,\n",
       " 'for': 1.5040773967762742,\n",
       " 'this': 1.2039728043259361,\n",
       " 'thank': 1.2878542883066382,\n",
       " 'and': 1.3862943611198906,\n",
       " 'i': 1.5040773967762742,\n",
       " 'my': 1.8325814637483102,\n",
       " 'all': 1.8325814637483102,\n",
       " 'in': 2.0794415416798357,\n",
       " 'be': 1.8325814637483102,\n",
       " 'who': 2.4423470353692043,\n",
       " 'world': 2.0794415416798357,\n",
       " 'very': 2.0794415416798357,\n",
       " 'have': 2.0794415416798357,\n",
       " 'by': 2.0794415416798357,\n",
       " 'we': 2.0794415416798357,\n",
       " 'our': 2.0794415416798357,\n",
       " 'is': 2.4423470353692043,\n",
       " 'not': 2.0794415416798357,\n",
       " 'people': 2.4423470353692043,\n",
       " 'out': 2.4423470353692043,\n",
       " 'so': 2.4423470353692043,\n",
       " 'much': 2.4423470353692043,\n",
       " 'year': 2.4423470353692043,\n",
       " 'revenant': 2.4423470353692043,\n",
       " 'was': 2.4423470353692043,\n",
       " 'off': 2.4423470353692043,\n",
       " 'tom': 2.4423470353692043,\n",
       " 'your': 3.091042453358316,\n",
       " 'screen': 3.091042453358316,\n",
       " 'a': 2.4423470353692043,\n",
       " 'entire': 2.4423470353692043,\n",
       " 'would': 2.4423470353692043,\n",
       " 'just': 2.4423470353692043,\n",
       " 's': 2.4423470353692043,\n",
       " 'collectively': 2.4423470353692043,\n",
       " 'planet': 2.4423470353692043,\n",
       " 'it': 2.4423470353692043,\n",
       " 'most': 2.4423470353692043,\n",
       " 'need': 2.4423470353692043,\n",
       " 'do': 2.4423470353692043,\n",
       " 'speak': 3.091042453358316,\n",
       " 'billions': 3.091042453358316,\n",
       " 'there': 2.4423470353692043,\n",
       " 'children': 3.091042453358316,\n",
       " 'tonight': 2.4423470353692043,\n",
       " 'take': 2.4423470353692043,\n",
       " 'granted': 2.4423470353692043,\n",
       " 'academy': 3.091042453358316,\n",
       " 'room': 3.091042453358316,\n",
       " 'congratulate': 3.091042453358316,\n",
       " 'other': 3.091042453358316,\n",
       " 'incredible': 3.091042453358316,\n",
       " 'nominees': 3.091042453358316,\n",
       " 'product': 3.091042453358316,\n",
       " 'tireless': 3.091042453358316,\n",
       " 'efforts': 3.091042453358316,\n",
       " 'an': 3.091042453358316,\n",
       " 'unbelievable': 3.091042453358316,\n",
       " 'cast': 3.091042453358316,\n",
       " 'crew': 3.091042453358316,\n",
       " 'first': 3.091042453358316,\n",
       " 'brother': 3.091042453358316,\n",
       " 'endeavor': 3.091042453358316,\n",
       " 'mr': 3.091042453358316,\n",
       " 'hardy': 3.091042453358316,\n",
       " 'talent': 3.091042453358316,\n",
       " 'on': 3.091042453358316,\n",
       " 'can': 3.091042453358316,\n",
       " 'only': 3.091042453358316,\n",
       " 'surpassed': 3.091042453358316,\n",
       " 'friendship': 3.091042453358316,\n",
       " 'creating': 3.091042453358316,\n",
       " 't': 3.091042453358316,\n",
       " 'ranscendent': 3.091042453358316,\n",
       " 'cinematic': 3.091042453358316,\n",
       " 'experience': 3.091042453358316,\n",
       " 'everybody': 3.091042453358316,\n",
       " 'at': 3.091042453358316,\n",
       " 'fox': 3.091042453358316,\n",
       " 'new': 3.091042453358316,\n",
       " 'regency': 3.091042453358316,\n",
       " 'team': 3.091042453358316,\n",
       " 'everyone': 3.091042453358316,\n",
       " 'from': 3.091042453358316,\n",
       " 'onset': 3.091042453358316,\n",
       " 'career': 3.091042453358316,\n",
       " 'parents': 3.091042453358316,\n",
       " 'none': 3.091042453358316,\n",
       " 'possible': 3.091042453358316,\n",
       " 'without': 3.091042453358316,\n",
       " 'friends': 3.091042453358316,\n",
       " 'love': 3.091042453358316,\n",
       " 'dearly': 3.091042453358316,\n",
       " 'know': 3.091042453358316,\n",
       " 'are': 3.091042453358316,\n",
       " 'lastly': 3.091042453358316}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in freq_words:\n",
    "    doc_tf=[]\n",
    "    for data in dataset:\n",
    "        frequency=0\n",
    "        for w in nltk.word_tokenize(data):\n",
    "            if w==word:\n",
    "                frequency+=1\n",
    "        word_TF=(frequency)/(len(nltk.word_tokenize(data)))\n",
    "        doc_tf.append(word_TF)\n",
    "    tf_matrix[word]=doc_tf\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix=[]\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf=[]\n",
    "    for value in tf_matrix[word]:\n",
    "        score=value*word_idfs[word]\n",
    "        tfidf.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document transform [[0.         0.         0.         ... 0.         0.28948527 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.36677345 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.49707942 0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.31610787 0.        ]]\n",
      "\n",
      "\n",
      "[[0.         0.         0.         0.         0.         0.40168022\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.48436405 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.48436405 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30653925 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43779893 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.28948527 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "arr1=X.toarray()\n",
    "print('Document transform',arr1)\n",
    "print(\"\\n\")\n",
    "print(arr1[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2015', 'able', 'about', 'academy', 'affected', 'all', 'amazing', 'an', 'and', 'are', 'around', 'as', 'at', 'award', 'be', 'been', 'big', 'billions', 'brother', 'but', 'by', 'can', 'career', 'cast', 'change', 'children', 'cinematic', 'climate', 'collectively', 'congratulate', 'creating', 'crew', 'dearly', 'do', 'drowned', 'efforts', 'endeavor', 'entire', 'everybody', 'everyone', 'experience', 'facing', 'felt', 'find', 'first', 'for', 'fox', 'friends', 'friendship', 'from', 'granted', 'greed', 'happening', 'hardy', 'have', 'history', 'hottest', 'humanity', 'in', 'incredible', 'indigenous', 'is', 'it', 'just', 'know', 'lastly', 'leaders', 'let', 'love', 'making', 'man', 'most', 'move', 'mr', 'much', 'my', 'natural', 'need', 'needed', 'new', 'nominees', 'none', 'not', 'now', 'of', 'off', 'on', 'only', 'onset', 'other', 'our', 'out', 'parents', 'people', 'planet', 'politics', 'polluters', 'possible', 'procrastinating', 'product', 'production', 'ranscendent', 'real', 'recorded', 'regency', 'relationship', 'revenant', 'right', 'room', 'say', 'screen', 'snow', 'so', 'southern', 'speak', 'species', 'stop', 'support', 'surpassed', 'take', 'talent', 'team', 'thank', 'that', 'the', 'there', 'this', 'those', 'threat', 'tip', 'tireless', 'to', 'together', 'tom', 'tonight', 'unbelievable', 'underprivileged', 'urgent', 'us', 'very', 'voices', 'want', 'was', 'we', 'who', 'whose', 'without', 'work', 'world', 'would', 'year', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-GRAM model\n",
    "\n",
    "import random\n",
    "\n",
    "text = \"\"\"Global warming or climate change has become a worldwide concern. \n",
    "It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, \n",
    "changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase\n",
    "in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s atmosphere.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "ngrams ={}\n",
    "\n",
    "#create the N grams\n",
    "\n",
    "for i in range(len(text)-n):\n",
    "    gram = text[i:i+n]\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram]=[]\n",
    "    ngrams[gram].append(text[i+n])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global warming glaciers, risis evide concerns, floods, cyclones into an in the earth due temperature of\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#testing our N-gram Model\n",
    "n=3\n",
    "currentGram=text[0:n]\n",
    "result = currentGram\n",
    "for i in range(100):\n",
    "    if currentGram not in ngrams.keys():\n",
    "        break\n",
    "    possiblities = ngrams[currentGram]\n",
    "    nextItem = possiblities[random.randrange(len(possiblities))]\n",
    "    result+=nextItem\n",
    "    currentGram=result[len(result)-n:len(result)]\n",
    "    \n",
    "print(result)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngram word model\n",
    "\n",
    "ngrams={}\n",
    "import nltk\n",
    "n=2\n",
    "#Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words)-n):\n",
    "    gram=' '.join(words[i:i+n])\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram]=[]\n",
    "    ngrams[gram].append(words[i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Global warming or': ['climate'],\n",
       " 'warming or climate': ['change'],\n",
       " 'or climate change': ['has'],\n",
       " 'climate change has': ['become'],\n",
       " 'change has become': ['a'],\n",
       " 'has become a': ['worldwide'],\n",
       " 'become a worldwide': ['concern'],\n",
       " 'a worldwide concern': ['.'],\n",
       " 'worldwide concern .': ['It'],\n",
       " 'concern . It': ['is'],\n",
       " '. It is': ['gradually'],\n",
       " 'It is gradually': ['developing'],\n",
       " 'is gradually developing': ['into'],\n",
       " 'gradually developing into': ['an'],\n",
       " 'developing into an': ['unprecedented'],\n",
       " 'into an unprecedented': ['environmental'],\n",
       " 'an unprecedented environmental': ['crisis'],\n",
       " 'unprecedented environmental crisis': ['evident'],\n",
       " 'environmental crisis evident': ['in'],\n",
       " 'crisis evident in': ['melting'],\n",
       " 'evident in melting': ['glaciers'],\n",
       " 'in melting glaciers': [','],\n",
       " 'melting glaciers ,': ['changing'],\n",
       " 'glaciers , changing': ['weather'],\n",
       " ', changing weather': ['patterns'],\n",
       " 'changing weather patterns': [','],\n",
       " 'weather patterns ,': ['rising'],\n",
       " 'patterns , rising': ['sea'],\n",
       " ', rising sea': ['levels'],\n",
       " 'rising sea levels': [','],\n",
       " 'sea levels ,': ['floods'],\n",
       " 'levels , floods': [','],\n",
       " ', floods ,': ['cyclones'],\n",
       " 'floods , cyclones': ['and'],\n",
       " ', cyclones and': ['droughts'],\n",
       " 'cyclones and droughts': ['.'],\n",
       " 'and droughts .': ['Global'],\n",
       " 'droughts . Global': ['warming'],\n",
       " '. Global warming': ['implies'],\n",
       " 'Global warming implies': ['an'],\n",
       " 'warming implies an': ['increase'],\n",
       " 'implies an increase': ['in'],\n",
       " 'an increase in': ['the'],\n",
       " 'increase in the': ['average'],\n",
       " 'in the average': ['temperature'],\n",
       " 'the average temperature': ['of'],\n",
       " 'average temperature of': ['the'],\n",
       " 'temperature of the': ['Earth'],\n",
       " 'of the Earth': ['due'],\n",
       " 'the Earth due': ['to'],\n",
       " 'Earth due to': ['entrapment'],\n",
       " 'due to entrapment': ['of'],\n",
       " 'to entrapment of': ['greenhouse'],\n",
       " 'entrapment of greenhouse': ['gases'],\n",
       " 'of greenhouse gases': ['in'],\n",
       " 'greenhouse gases in': ['the'],\n",
       " 'gases in the': ['earth'],\n",
       " 'in the earth': ['’'],\n",
       " 'the earth ’': ['s'],\n",
       " 'earth ’ s': ['atmosphere'],\n",
       " '’ s atmosphere': ['.']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or\n"
     ]
    }
   ],
   "source": [
    "currentGram=' '.join(words[0:n])\n",
    "result=currentGram\n",
    "for i in range(30):\n",
    "    if currentGram not in ngrams.keys():\n",
    "        break\n",
    "    possiblities=ngrams[currentGram]\n",
    "    nextItem = possiblities[random.randrange(len(possiblities))]\n",
    "    result = ' '+nextItem\n",
    "    rwords=nltk.word_tokenize(result)\n",
    "    currentGram=' '.join(rwords[len(rwords)-n:len(rwords)])\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
